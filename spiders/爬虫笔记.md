##爬虫笔记

1. urllib模块中的request

   ```
   from urllib import request
   url = '请求的链接'
   # 一个简单的响应 结果是bytes''串
   response = request.urlopen(url)
   html= response.read().decode('utf-8')#html语言需要解析下

   #构造请求头
   req = request.Request(url=,data=,headers=)
   response = request.urlopen(req)
   ```

2. urllib模块中的parse

   ```
   POST请求
   构造请求头的data时候
   data = {
     '':'',
     '':''
   }
   # data必须编码后才可传入
   data = parse.urlencode(data,encoding='GB2312  UTF-8 等选择一个')
   # 然后修改下请求头
   headers = {
     "Content-Length":len(data)
   }
   # 重构造请求头
   req = request.Request(url=login_url,data=bytes(data,encoding='GB2312'),headers=header)
   ```

3. http模块中的cookiejar  # 爬虫 2018-1-04 下午 11111.py  4 renrenlogin

   ```
   from urllib import request
   from http import cookiejar
   # 制作存储cookie对象
   cookie = cookiejar.CookieJar()
   # 制作一个cookie管理器
   cookie_handler = request.HTTPCookieProcessor(cookie)
   # 请求管理器
   opener = request.build_opener(cookie_handler)
   # 加载opener  后续request中就含有cookie 可以继续访问了
   request.install_opener(opener)
   ```

4. https

   ```
   # 忽略https 请求的证书校验
   import ssl
   ssl._create_default_https_context = ssl._create_unverified_context
   ```

5. cookies直接登录  #新浪微博

   ```
   head = {
     这里填写在浏览器中登录账号后   F12检查 查找cookies填进来
   }
   # 构造请求头
   req = request.Request(url=url,headers=head)
   ```

6.  使用request做post 请求 # 例子 2018-1-04下午3_post.py

   ```
   在请求头中  依旧是用parse.urlencode()
   ```

7.  代理 proxy

   ```
   proxy = {
     'http': 'http://{}:{}'.format(temp['ip'], temp['port']),
     'https': 'http://{}:{}'.format(temp['ip'], temp['port'])
           }
   # 代理
   proxy_handler = request.ProxyHandler(proxy)
   opener = request.build_opener(proxy_handler)
   opener.handlers = proxy_handler
   ```

8.  一些编码问题

   ```
   ***.decode('可以填utf-8 gb2312 gbk','ignore') ignore会忽略无法解码的字符  表情符号等等
   ```

9. json模块

   ```
   dumps是将dict转化成str格式，loads是将str转化成dict格式。

       json.loads()用法:
             # 类似json格式的数据在字符串中
             str = '[{"":""},{"":""}]'
             json_data = json.loads(str)
       json.dumps()用法:
       # 注 !! 第二个参数 ensure_ascii=False  防止把汉字转为ascii码
       # indent = 数字 或者 None  表示缩进  漂亮展示
             # 以下是我写出json文件的方法
             f = open('美丽说首页商品.json','a')
             f.write('[\n')
             # item是字典~
             for item in data_list:
                 f.write(json.dumps(item,ensure_ascii=False,indent=None))
                 f.write(',\n')
             f.write(']')

   dump和load也是类似的功能，只是与文件操作结合起来了。

       json.load()用法:
             f = open('IP_6622.json','r')
             temp = json.load(f)
             f.close()
             # temp中存的是 [{"":""},{"":""}]

       json.dump()用法:
             a = {'name': 'wang'}
             fp = file('test.txt', 'w')
             json.dump(a, fp)
   ```

10. 正则表达式 re模块

  ```
  主要功能：
      1.校验功能，账户 密码 电话 邮箱 ip地址
      2.在目标字符串中匹配想要的字符

  常用方法：
      search
      match : 默认在规则加^
      findall :

  常用规则
      \d : 匹配一个数字
      \w : 匹配一个数字字母下划线
      \s : 匹配空格

      \D : 匹配非 \d
      \W : 匹配非 \w
      \S : 匹配非 \s

      .: 匹配任意字符,但是不能匹配换行 ,如果加上 re.S 则匹配换行符
      .*? : 非贪婪模式
      .* : 贪婪模式
      	exp:
      		re.compile(r'\d*') 匹配数字第一个是数字就继续匹配下去,直到不是数字为止
      		re.compile(r'\d*?') 第一个是数字也不匹配...为啥啊?!!!
      * : 匹配 0 - N 个*号前边的字符
      + : 匹配 1 - N 个+号前边的字符
      ? : 匹配 0 - 1 个?号前边的字符

      [] : 所有字符都是原字符（无特殊含义）
      [^] : 非 []
      {n} : 重复 N次
      {m,n} : 重复 m 到 n 次
      () : 分组

      ^ : 必须以规则开始
      $ ：必须以规则结束
  ```

11. jsonpath 模块

    ```
    JSONPath	描述
    $			根节点
    @			现行节点
    .or[]		取子节点
    ..			就是不管位置，选择所有符合条件的条件
    *			匹配所有元素节点
    []			迭代器标示（可以在里边做简单的迭代操作，如数组下标，根据内容选值等）
    [,]			支持迭代器中做多选。
    ?()			支持过滤操作.
    ()			支持表达式计算
    ```

    - res = jsonpath.jsonpath(json.loads(这里写json格式字符串),'这里写规则')
    - res 的类型是列表

12. xpath模块

   ```
   /				 根节点
   //				 所有节点
   *				 匹配任何节点
   @				 属性选择
   @*				 匹配所有属性节点
   node()			  匹配任何类型的节点。
   text()			  内容
   .				 当前节点
   ..				 当前节点父节点
   book[1]		      前面规则下的第一个book元素
   book[last()]	  规则下最后一个 book 元素
   book[position()<3] 选取最前面的两个属于 bookstore 元素的子元素的 book 元素。
   title[@lang]	  选取所有拥有名为 lang 的属性的 
   ```

   用法详见 zufang58.py

13. bs4模块

   ```
   from bs4 import BeautifulSoup
   html = BeautifulSoup(html,'lxml')# 将html处理下 如果安装lxml就写lxml,看系统提示
   使用:
   	html.标签名  >> 得到标签
   	html.标签名.name >> 标签名字
   	html.标签名.string >> 标签内容
   函数:
   	html.find_all('标签名')  >>  返回一个列表  是所有要查找的标签
   	标签名.get_text()   >> 标签下的所有文本
   CSS选择器(以下结果都为列表):
   	.class选择:
         html.select('p.title')  >> 筛选  class属性带有title的p标签
         html.select('p .b')   >> 筛选 p标签子孙节点中  class带有b 的标签
         html.select('p > .b')  >> p下的子节点中 class带有b 的标签
       #id选择器:
         html.select('p#p1')  >> 带有id属性是p1的 p标签
       或者:
   	  html.select('条件a,条件b,条件c') >> 满足的都筛选出来
       根据属性筛选:
   	  html.select("div[class='story1']") >> 筛选class带有story1 的div
   	  html.select("div[class^='a']") >>  筛选class带有以a开头的属性 的div
   	  html.select("div[class$='b']") >>  筛选class带有以b末尾的属性 的div
   	  html.select("div[class*='b']") >>  筛选class  含有b	的属性 的div
   ```

14. requests模块[request介绍](http://www.gaoxuewen.cn/index.php/python/1086.html)

   ```
   response = requests.get(url=,stream=,timeout=,proxies=)
   	timeout 超时设置 有效的连接
   	stream 迭代器 
   POST(python2 的用法 3 还没测!!):
   	r = requests.Session()
   	r.get('url')
   	token = r.cookies["_csrf_token"]
   	d = {
         "_csrf_token":token,
         'username': ,
         'password': ,
   	}
   	res = r.post('url',data = d)

   html = response.text  # 查看返回html代码
   url = response.url # 查看访问的url
   status_code = response.status_code # 返回的状态码
   content = response.content # 以字节的方式相应内容

   json_data = response.json() # 内置的json解码器

   response.headers # 以字典格式展示 服务器响应头
   response.request.headers # 查看请求头
   response.headers.get('content-type') # 类似字典取值
   response.cookies['example_cookie_name'] #查看cookies信息 键值对
   response.history #请求历史
   ```

15. ​ 进程 

   ```
   优点：提高效率，利用cpu多核优势
   开启个数：理论上是cpu内核的1-2倍
   描述：一段程序或者脚本的执行，cpu资源分配的最小单位
   缺点：资源消耗非常大，进程过多，cpu切换进程执行也消耗资源，资源共享困难
   使用场景：cpu密集型应用程序（计算密集型）
   ```

   ```
   注意几个要点:
   	multiprocessing.Process(target=foo,args=(,))
   	foo 是函数名  函数要接收的参数在后面的args中 必须以元组的方式写入
   	
   start是开始进程
   join是阻塞进程的方法
   进程池:
   from multiprocessing import Pool
   ```

   案例1:

   ```
   import multiprocessing
   # 定义进程函数
   def foo(num):
       print('hello %d号进程' % num)

   # 在windows下如果创建多进程则需要在以下条件成立
   if __name__ == '__main__':
       # 创建一个进程，则需要先定义一个函数
       # target ： 进程函数对象
       # args ： 进程函数的参数
       for i in range(10):
           p = multiprocessing.Process(target=foo,args=(i,))
           p.start()
   ```

   案例2:

   ```
   import multiprocessing  # 导入多进程模块
   from multiprocessing import Manager

   # 定义进程函数
   def foo(num,ls):
       ls.append(num)
       print('hello %d号进程' % num)

   # 在windows下如果创建多进程则需要在以下条件成立
   if __name__ == '__main__':
       # 创建一个进程，则需要先定义一个函数
       # target ： 进程函数对象
       # args ： 进程函数的参数
       m = Manager() # 初始化一个manager对象
       ls = m.list() # 创建共享数据类型

       process_list = []
       for i in range(10):
           p = multiprocessing.Process(target=foo,args=(i,ls))
           p.start() # 启动一个进程
           process_list.append(p) # 进程执行中会阻塞主进程

       for p in process_list:
           p.join()

       # 需要等待10个进程全部执行完毕再继续执行主进程
       print('进程执行完毕，执行我')
       print(ls)
   ```

   案例3:

   ```
   # 1-50-1号进程   51-100-2号进程
   import time
   # 进程池
   from multiprocessing import Pool

   def foo(num):
       time.sleep(1)
       print(num)

   if __name__ == '__main__':
       # 创建一个进程池
       pool = Pool(16)
       for i in range(100):
           # 创建一个进程
           pool.apply_async(func=foo,args=(i,))
       # 进程池中所有进程执行完毕，继续执行
       pool.close()
       pool.join()
   ```

   案例4: 多进程爬取腾讯招聘信息

   ```
   from multiprocessing import Pool
   import requests
   from bs4 import BeautifulSoup
   import time
   from queue import Queue
   import re

   base_url = 'http://hr.tencent.com/position.php?start=%d'

   headers = {
       'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36'
   }

   def getPage(i):
       fullurl = base_url % i
       response = requests.get(url=fullurl,headers=headers)
       html = response.text

       # 获取详情页链接地址
       html = BeautifulSoup(html, 'lxml')
       tr_list = html.select('table.tablelist tr')[1:-1]
       for tr in tr_list:
           name = tr.select('td a')[0].text
           print(name)

   if __name__ == '__main__':
       pool = Pool(8)
       # 创建进程
       print(time.ctime())
       for i in range(0,990 + 1,10):
           pool.apply_async(func=getPage, args=(i,))

       pool.close()
       pool.join()
       print(time.ctime())
   ```

16. 线程

   ```
   优点：提高效率，资源共享
   开启个数：跟计算硬件有关系，跟应用场景有关系，一般高于可开启进程守数
   描述：进程下可以开启多个线程，cpu调度的最小单位
   缺点：开启个数也不是无限的，如果开启过多，造成进程瘫痪
   使用场景：IO（input（response），ouput（request），）密集型应用程序，爬虫成是网络IO，长延时操作
   ```

   线程锁: threading.Lock()

   ```
   比如多个线程同时操作数据库的增删改查会出错  为每个线程执行删除操作前 加上一个线程锁
   ```

   ​

   ```
   跟进程类似:
   	start 开始 
   	join  阻塞
   	.is_alive 判断线程状态
   ```

   案例1:

   ```
   import threading # 导入线程模块
   import time

   # 线程函数
   def foo(num):
       time.sleep(2)
       print(num)

   # 创建一个线程
   t1 = threading.Thread(target=foo,args=(1,))
   t2 = threading.Thread(target=foo,args=(2,))

   # 启动线程
   t1.start()
   t2.start()
   ```

   案例2:

   ```
   import threading
   import time
   def foo(i):
       print('%d号线程启动' % i)
       time.sleep(2)
       print('%d号线程结束' % i)

   thread_list = []
   for i in range(1,10):
       t = threading.Thread(target=foo,args=(i,))
       t.start()
       print(t.is_alive())
       thread_list.append(t)

   # 等待所有线程执行完毕
   for t in thread_list:
       t.join()

   # 查看所有线程状态
   for t in thread_list:
       print(t.is_alive())
   ```

   案例3:用类继承的方法写

   ```
   import threading
   class MyThread(threading.Thread):
       def __init__(self):
           # 找到MyThread的父类然后调用init方法
           super(MyThread, self).__init__()
           pass

       # 在线程启动的时候被调用
       def run(self):
           print('启动线程')

   if __name__ == '__main__':
       # 实例化相当于创建一个线程
       t = MyThread()
       t.start()
   ```

17. selenium

   ```
   模拟谷歌浏览器:
       from selenium import webdriver
       # 制作一个浏览器 这里选择谷歌浏览器  需要提前把谷歌浏览器的驱动添加在环境变量中  或者
       # 在 Chorme(executable_path="路径")
       browser = webdriver.Chrome()
       browser.get(url) # 访问网页
       html = browser.page_source # 获取网页代码
       browser.save_screenshot() # 保存截图
       # 模拟瀑布流 向下滚动  document.body.scrollHeight 浏览器页面高啊
       browser.execute_script('scrollTo(0,document.body.scrollHeight)')
     
       browser.find_element_by_class_name() # 查询方法很多  配合lxml使用
       browser.quit() # 关闭浏览器
     
   模拟PhantomJS(小的浏览器 bug多 要加请求头等等):
       from selenium import webdriver
       import time
       browser = webdriver.PhantomJS()
       browser.get("https://www.baidu.com")
       time.sleep(10)
       browser.save_screenshot('baidu.png')
       browser.quit()
   ```

18.  Scrapy 框架

   1. 安装

      ```
      mac:
      	pip install scrapy

      windows:
      	利用国内源
      	pip install -i https://pypi.tuna.tsinghua.edu.cn/simple scrapy
      	
      	如果失败则先手动安装twisted
      	pip install 目录\Twisted-17.9.0-cp35-cp35m-win_amd64.whl
      	然后安装
      	pip install scrapy
      	安装pywin32
      ```

   2. 创建项目

      ```
      scrapy startproject 项目名
      cd 项目名 
      scrapy genspider 爬虫名称 网站
      开启项目
      settings 中可以吧 robot 置为False (robots.txt 爬虫协议 大网站基本都有)
      在 spiders 文件夹下的 上述爬虫名称.py文件中写 请求  解析 等代码
      ```

   3. setting.py

      ```
      ROBOTSTXT_OBEY = False  # 是否遵循robot.txt协议的开关
      CONCURRENT_REQUESTS = 32 # 并发量设置
      DOWNLOAD_DELAY = 3 # 下载延迟 防止频繁访问被封IP  延迟时间是 0.5至1.5倍 乘 3 秒

      还可以写一些通用的变量 在其他文件中导入 方便修改

      ```

   4. items.py

      ```
      可以填写要处理的字段
      class Product(scrapy.Item):
          name = scrapy.Field()
          price = scrapy.Field()
          stock = scrapy.Field()

      在spider爬虫文件中 引用该文件  实例化对象 加入数据
      ```

   5. middleware.py

      ```
      访问页面的时候  需使用 下载模块 在这里可以写代理
      # 下载中间件  数字表示优先级 越低越优先 
      DOWNLOADER_MIDDLEWARES = {
          'jobbole.middlewares.JobboleDownloaderMiddleware': 543,
      }
      ```

   6. pipelines.py

      ```
      # 图片处理管道
      from scrapy.pipelines.images import ImagesPipeline
      ```

   7. spider.py

      ```
      爬虫抓取页面都写在这里
      response.status 状态码
      response.xpath() 填写匹配规则
      response.xpath().extract() 抽取与页面中
      ```

   8.  运行

      ```
      1. 可以再命令行中输入 spider crawl spidername
      2. 创建一个py文件(main.py):和scrapy.cfg同级目录
      	from scrapy import cmdline
      	# 字符串不能被执行  切一下
      	cmdline.execute('scrapy crawl spidername'.split())
      ```

   9.  图片/文件 下载

      ```
      可以再setting中写
      IMAGES_STORE = '存储路径'
      # 第一个是项目名称.
      ITEM_PIPELINES = {
          'image_download.pipelines.ImageDownloadPipeline': 1,
      }

      或者在 爬虫文件中写 custom_settings 设置
      custom_settings={
         'IMAGES_STORE': 存储路径,
         'ITEM_PIPELINES':{
         'image_download.pipelines.ImageDownloadPipeline': 1,
         },
         
      选择完管道后 在管道中 也要处理
      class ImageDownloadPipeline(ImagesPipeline):
          def get_media_requests(self,item, info):
              for img_url in item['img_urls']:
                  yield scrapy.Request(url=img_url)

          def item_completed(self, results, item, info):
              image_paths = [x['path'] for ok, x in results if ok]
              if not image_paths:
                  raise DropItem("Item contains no images")
              item['image_paths'] = image_paths
              return item
              
      item中设置
      class ImageDownloadItem(scrapy.Item):
          img_urls = scrapy.Field() # 在爬虫文件中 要是一个列表 里面放图片链接
          image_paths = scrapy.Field()
          
      spider 中 :
      def get_image_link(self,response):
          item = ImageDownloadItem()
          item['img_urls'] = img_urls # 这个必须是列表!!!!!!!!
          yield item
      ```

   10. ​ pipeline中执行写入数据库操作

     ```
     class XXXXXXpipelines(object):
         def __init__(self):
             try:
                 self.conn = pymysql.connect('127.0.0.1', 'root', '密码', '数据库名', charset='utf8')
                 self.cursor = self.conn.cursor()
             except Exception as e:
                 print(e)
                 
     	def process_item(self,item,spider):
     		sql = ''
     		data = ''
     		        try:
                 self.cursor.execute(sql, data)
                 self.conn.commit()
             except Exception as e:
                 print(e)
                 self.conn.rollback()
         def close_spider(self,spider):
             self.cursor.close()
             self.conn.close()
     ```

   11. FormRequest  账号密码登陆人人网

      ```
      class RenSpider(scrapy.Spider):
          name = 'ren'
          allowed_domains = ['renren.com']

          def start_requests(self):
              login_url = 'http://www.renren.com/PLogin.do'
              data = {
                  'email' : '账号',
                  'password' : '密码',
              }
              headers = {}
              yield scrapy.FormRequest(url=login_url,formdata=data)
      ```

   12. Cookie登陆

       ```
       class XXXXXSpider(scray.Spider):
       	name = 'XXXXX'
           allowed_domains = ['xxxxxxxxxxxxxxxxxx']
           start_urls = ['http://www.baidu.com/']
           headers = {
             XXXXXXXXXXXXX
             # COOKIE不写在这里  !!!!!!!!  自己写单独的脚本可以在这里写 注意区别
           }
           #在页面中获取的 请求头是 这样的格式
           # Cookie : a= 1 ; B = 2; `````````````
           cookie = {
             'a' : '1',
             'b' : '2',
           }
           
           def parse(self,response):
           	url = '基于cookie登陆的页面`'
           	yield 		scrapy.Request(url=url,headers=self.headers,cookies=self.cookie,callback=self.XXXX)
       ```

   13. CrawlSpider

       ```
       EXP:
       	scrapy genspider -t crawl cnblogs cnblogs.com

       class CnblogsSpider(CrawlSpider):
           name = 'cnblogs'
           allowed_domains = ['cnblogs.com']
           start_urls = ['https://www.cnblogs.com/']
       	
       	# allow写正则匹配规则  follow = True  继续进入当前正则匹配到的页面 
           rules = (
               Rule(LinkExtractor(allow=r'/cate/\w+/'),follow=True),
           )

           def parse_item(self, response):
              nick = response.xpath('//a[@id="Header1_HeaderTitle"]/text()')
              print(nick)
       ```

   14. ​

19. 拓展模块

   1.  uuid

      ```
      import uuid
      uuid.uuid4()
      随机生成一个  UUID('7f03fbe1-7a6b-4e70-99ba-0069a4cf339a') 重复概率几乎为0
      ```

   2. hashlib 加密

      ```
      import hashlib
      md5 = hashlib.md5()
      md5.update(b'')  # 或者 md5.update(bytes(字符串等,encoding='utf-8'))
      md5.hexdigest() # 十六进制的
      ```

   3. datetime 模块

      ```
      a = timedelta(days=2)        >>> datetime.timedelta(2)
      b = datetime.datetime.now()  >>> datetime.datetime(2018, 1, 22, 22, 39, 12, 618175)
      res = b - a                  >>> datetime.datetime(2018, 1, 20, 22, 39, 36, 504063)
      res.strftime('%Y/%m/%d')     >>> '2018/01/20'
      ```

   4. ​

   5. ​

20. 分布式爬虫

   1. settings  确保redis服务开启 可以连接!!!

      ```
      # 过滤系统设置为scrapy-redis的过滤器
      DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"

      # 调度器
      SCHEDULER = "scrapy_redis.scheduler.Scheduler"

      # 是否可以暂停爬虫
      SCHEDULER_PERSIST = True

      # 请求队列模式  只能选择一个
      # 按优先级调度请求,priority 数字越大，优先级越高
      SCHEDULER_QUEUE_CLASS = "scrapy_redis.queue.SpiderPriorityQueue"

      # 先进先出队列
      #SCHEDULER_QUEUE_CLASS = "scrapy_redis.queue.SpiderQueue"

      # 先进后出
      #SCHEDULER_QUEUE_CLASS = "scrapy_redis.queue.SpiderStack"

      # 数据存储到redis中  管道优先级数字越小 越优先
      ITEM_PIPELINES = {
      'example.pipelines.ExamplePipeline': 300,# 自己写管道处理
      'scrapy_redis.pipelines.RedisPipeline': 999,# 类scrapy_redis中继承的
      }

      # redis连接信息
      REDIS_HOST = '服务器的地址'
      REDIS_PORT = 6379
      ```

   2. 导入scrapy_redis 类    设置属性  等等见3

      ```
      # 两种类
      from scrapy_redis.spiders import RedisSpider
      from scrapy_redis.spiders import RedisCrawlSpider
      # 属性
      redis_key
      ```

   3. 分类  其中第一个不是 分布式

      ```
      1. CrawlSpider
        from scrapy.spiders import CrawlSpider, Rule
        class DmozSpider(CrawlSpider):
            name = 'hao123'
            allowed_domains = []
            # 必须写 start_urls
            start_urls = ['http://www.163.com/']
            rules = [
                Rule(LinkExtractor(), callback='parse', follow=),
            ]
            def parse(self,response):
                pass
      2. RedisSpider
      from scrapy_redis.spiders import RedisSpider
      import scrapy
      class MySpider(RedisSpider):
          name = 'myspider_redis'
          # redis 中需要向redis数据库中插入开始页面
          # lpush myspider:start_urls http://www.hao123.com
          redis_key = 'myspider:start_urls'
          def parse(self, response):
            pass
      3. RedisCrawlSpider
      from scrapy.spiders import Rule
      from scrapy.linkextractors import LinkExtractor
      from scrapy_redis.spiders import RedisCrawlSpider
      class MyCrawler(RedisCrawlSpider):
          name = 'mycrawler_redis'
          redis_key = 'mycrawler'
          rules = (
          	Rule(LinkExtractor(), callback='', follow=True),
          )
          def parse(self, response):
              return {
                  'url': response.url,
              } # item
      ```

21. sa

22. ​


```
  1.spiders文件夹下共有6个爬虫文件 分别是对6个招聘网站的爬取  均是全站爬取 job1中的cookie需登录账号查看后填写
  
  2.几个main文件是对爬虫文件的执行  可以同时多开
  
  3.settings中需要再设置redis服务器的地址
```



